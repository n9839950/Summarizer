# Datasets
Please download the preprocessed datasets files `summarizer_dataset_*.h5` [here](https://drive.google.com/open?id=1sbZZalh43n6fiSxWt_SIGgv72bt4rdoG) and place them here. Alternatively you can run `python download_datasets.py` to download them automatically.

## Documentation
Every `.h5` file has the following structure:
```
/key
    /features
        - 2D-array with shape (n_steps, feature-dimension)
        - Feature representation of the frames, extracted from a pretrained CNN
        - It is the input to most of the models
    /user_summary
        - 2D-array with shape (num_users, n_frames), each row is a binary vector
        - This is used in evaluation for correlation and F-score
    /gtscore
        - 1D-array with shape (n_steps), stores ground truth importance scores (e.g. [.3 .8 ... .1])
        - This is the target to predict in supervised models
        - This can be used for maximum likelihood loss
    /gtsummary
        - 1D-array with shape (n_steps), ground truth summary (e.g. [0 1 ... 0])
    /user_scores
        - 2D-array with shape (num_users, n_steps), scores used for correlation evaluation
        - This should not be used for training
    /change_points
        - 2D-array with shape (num_segments, 2), each row stores a segment and its frame indices (start, end)
        - This is typically the result of KTS on the original video
        - It is used in summary generation
    /n_frame_per_seg
        - 1D-array with shape (num_segments), indicates number of frames in each segment
        - It is used in summary generation
    /n_frames
        - Integer, number of frames in original video
    /n_steps
        - Integer, number of subsampled frames
    /picks
        - 1D-array with shape (n_steps), positions of subsampled frames in original video
        - This is a mapping where each element of /picks is the frame position in original video
        - So min(picks) = 0 and max(picks) = n_frames-1
        - It is used for evaluation and summary generation
    /video_name
        - String, original video name
```

Further details:
* `/gtscore` and `/gtsummary` are used for training. Details about them are given for each dataset below.
* `n_steps` is the number of frames to predict. The original number of frames is `n_frames`. For computational reasons, the frames were subsampled to `n_steps` for training.
* `feature-dimension` is for instance 1024 as the frames were forwarded through GoogleLeNet up to layer pool5 which is of length 1024.
* `/user_scores` is used for scores correlation evaluation, and depends on the dataset, see sections below for details.

### SumMe
File is `summarizer_dataset_summe_google_pool5.h5`.
> SumMe was released as a Video Summarization benchmark dataset, publicly available, human annotated, for frame importance score prediction. It contains a set of 25 general-purpose public videos from YouTube. Each video has 15 to 18 human annotations. They were asked to create a summary having the most important content with a limited time length. Thus, the annotations are 0/1 values for each frame of the videos indicating if a particular frame was picked by the annotator to be a part of his summary. As a target for the models, importance frame scores are derived from the proportion of annotators who picked the frames.Most of the scores are between 0.0 and 0.5.

The original [SumMe](https://gyglim.github.io/me/vsum/) annotations are summaries `[0 1 1 ... 0]` (not frame importance scores). They are stored in `/user_summary`. They were used to generate `/gtscore`, `/gtsummary`, `/user_scores` as follows:

* `/gtscore` comes from the original data in SumMe. It was computed as the frequency for each frame being selected by annotators. In other words, `gtscore(frame) = nb of times the frame is selected by annotators / nb of annotators`.

* `/gtsummary` was computed by [dppLSTM](https://github.com/kezhang-cs/Video-Summarization-with-LSTM) with KTS+Knapsack with 15% time constraint on `/gtscore`.

* `/user_scores` was generated by us. For SumMe, this is the same as `/gtscore`.

### TVSum
File is `summarizer_dataset_tvsum_google_pool5.h5`.
> TVSum was later introduced for Video Summarization models validation. It contains 50 general-purpose videos from YouTube as well, annotated by 20 humans. In this dataset, the annotations were designed differently: annotators were asked to give a score between 1 (uninteresting) and 5 (interesting) to short video segments of two seconds. The importance frame scores to be predicted are derived by setting the score to all frame within the two seconds segment. They are also rescaled in [0,1] using min-max normalization.

The original [TVSum](https://github.com/yalesong/tvsum/) annotations are importances scores in [1, 5] given by 20 people for each video. These score annotations were used to generate `/user_summary`, `/gtscore`, `/gtsummary`, `/user_scores` as follows:

* `/user_summary` was computed by [dppLSTM](https://github.com/kezhang-cs/Video-Summarization-with-LSTM) (see [supplementary material](https://arxiv.org/abs/1605.08110), §1.3 and §2.2). They used the original annotations scores of TVSum and then used KTS+Knapsack with a length constraint of 15%.

* `/gtscore` was provided in the original [TVSum data](https://github.com/yalesong/tvsum/blob/master/matlab/ydata-tvsum50.mat). The annotators gave a note between 1 and 5 to frames. The notes were averaged over annotators before being normalized to [0,1] with min-max normalization.

* `/gtsummary` just like SumMe, this was computed by [dppLSTM](https://github.com/kezhang-cs/Video-Summarization-with-LSTM) with KTS+Knapsack with 15% time constraint on `/gtscore`.

* `/user_scores` was generated by us. This is the original score annotations matrix but with scores normalized to [0,1].

## Original videos and frames
Download the original videos of the datasetsand place them as follows:
```
├── videos
│   ├── summe
│   │   ├── Air_Force_One.mp4
│   │   ├── ...
│   ├── tvsum
│   │   ├── _xMr-HKMfVA.mp4
│   │   ├── ...
```
You can extract the frames of the videos by using the `videos2frames.{sh,bat}` scripts. Please make sure you have [FFmpeg](https://www.ffmpeg.org) installed. They will be saved as follows:
```
├── videos
│   ├── summe
│   │   ├── Air_Force_One.mp4
│   │   ├── ...
│   │   ├── frames
│   │   │   ├── Air_Force_One
│   │   │   │   ├── 000001.jpg
│   │   │   │   ├── ...
│   ├── tvsum
│   │   ├── _xMr-HKMfVA.mp4
│   │   ├── ...
│   │   ├── frames
│   │   │   ├── _xMr-HKMfVA
│   │   │   │   ├── 000001.jpg
│   │   │   │   ├── ...
```

## From `eccv16_dataset_*.h5` to `summarizer_dataset_*.h5`
The dataset files we used in this repository are modified and improved versions of the files introduced by [dppLSTM](https://github.com/kezhang-cs/Video-Summarization-with-LSTM) and used by [DR-DSN](https://github.com/KaiyangZhou/pytorch-vsumm-reinforce), [VASNet](https://github.com/ok1zjf/VASNet) among others.

You can regenerate the `summarizer_dataset_*.h5` from the `eccv16_dataset_*.h5` by reading and executing the script `normalize_datasets.py`.
